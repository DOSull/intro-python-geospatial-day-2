{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e765af3",
   "metadata": {},
   "source": [
    "# Tabular operations in `pandas`\n",
    "We've already seen a simple _table join_ operation when we used the SA1 codes as an index to select only SA1 data for the Wellington urban area. In this notebook we look at the various ways that tables can be combined in `pandas` a bit more closely.\n",
    "\n",
    "## Joining tables with `merge()`\n",
    "As an example here are data pertaining to airports sourced via https://ourairports.com/data/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59604f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_all = pd.read_csv(\n",
    "    \"https://davidmegginson.github.io/ourairports-data/airports.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b29f83f",
   "metadata": {},
   "source": [
    "Let's cut things down so we are only dealing with airports in New Zealand that are significant enough to have an IATA code, and also slim the data down to only columns we really care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef60dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_nz = airports_all[(airports_all.iso_country == \"NZ\") &\n",
    "                        (airports_all.iata_code.notna())]\n",
    "airports_nz = airports_nz[\n",
    "    [\"name\", \"latitude_deg\", \"longitude_deg\", \"iata_code\"]]\n",
    "airports_nz = airports_nz \\\n",
    "    .rename(columns = {\"latitude_deg\": \"lat\", \"longitude_deg\": \"lon\"})\n",
    "airports_nz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e9ccf0",
   "metadata": {},
   "source": [
    "Rather out of date (but free!) information about scheduled flights is available via https://openflights.org/data.php. Per the information on that page, there are no column names in the data, so we have to tell `read_csv()` that using `header = None` and then cut the data down to the columns we want (the origin and destination IATA codes) and name them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eef2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = pd.read_csv(\"https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat\", header = None)\n",
    "schedule = schedule.iloc[:, [2, 4]]\n",
    "schedule.columns = [\"iata_code_1\", \"iata_code_2\"]\n",
    "schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c84c828",
   "metadata": {},
   "source": [
    "Now we want to associate with each scheduled flight within New Zealand the latitude-longitude of its respective airports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85381343",
   "metadata": {},
   "source": [
    "We do such a table join using the `DataFrame.merge()` method. There are a few ways to approach this. We'll see the cleanest approach at the end of this section. For now we'll wander through some messier approaches to show how the `merge` function works.\n",
    "\n",
    "We have to tell the `merge` function which column names we want to perform the join using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19042d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule.merge(airports_nz, left_on = \"iata_code_1\", right_on = \"iata_code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0866ad85",
   "metadata": {},
   "source": [
    "The result of this is what we want alright. The `lat` and `lon` attributes for the first airport in each case have been joined as we wanted. Now if we want to join the second airport code we can do the same again, but setting `iata_code_2` as the `left_on1` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15098575",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule \\\n",
    "    .merge(airports_nz, left_on = \"iata_code_1\", right_on = \"iata_code\") \\\n",
    "    .merge(airports_nz, left_on = \"iata_code_2\", right_on = \"iata_code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6611a",
   "metadata": {},
   "source": [
    "Things are now a bit messy because `pandas` tries to resolve issues with duplicate column names by adding default name suffixes `_x` and `_y`. That works OK, but can quickly get rather confusing. There are options in the `merge` function to specify different suffixes which we might want to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule \\\n",
    "    .merge(airports_nz, left_on = \"iata_code_1\", right_on = \"iata_code\") \\\n",
    "    .merge(airports_nz, left_on = \"iata_code_2\", right_on = \"iata_code\", \n",
    "           suffixes = [\"_orig\", \"_dest\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61b3580",
   "metadata": {},
   "source": [
    "That's a little bit nicer, although really, we would probably want to do quite a lot of dropping of duplicate columns and renaming. The promised much cleaner approach is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ab73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nz_scheduled_flights = schedule \\\n",
    "    .merge(airports_nz.add_suffix(\"_1\")) \\\n",
    "    .merge(airports_nz.add_suffix(\"_2\")) \\\n",
    "    .drop(columns = [\"name_1\", \"name_2\"])\n",
    "nz_scheduled_flights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b271614c",
   "metadata": {},
   "source": [
    "`add_suffix()` adds `\"_\"` to all the column names in `airports` before the merge operation. Because that will make `iata_code` into `iata_code_1` which matches the name of column on which we want to base the join in the `schedule` table we no longer have to specify `left_on` and `right_on` (`pandas` finds the matching names and uses them) or worry about suffixes. The same thing happens in the second merge operation. An additional benefit of this approach is that after the join has happened we retain only one copy of the `iata_code_*` columns.\n",
    "\n",
    "With careful consideration of column names it is often possible to perform table joins like this more cleanly than if you just charge ahead without much forethought!\n",
    "\n",
    "### Left, right, inner, outer, and cross joins\n",
    "By default `merge()` performs an **inner** join. This means that only rows where a match was found between the two tables will be retained. That's why the `schedule` data got cut down to size from an initial 67,663 routes to only 114 routes internal to New Zealand. After the first join only 218 routes remain in the data (only 218 scheduled routes had `iata_code_1` matching a New Zealand airport IATA code in the `airports` data). The second join slimmed this down further to only 114 routes where both codes are matched.\n",
    "\n",
    "An **outer** join retains all rows from both tables regardless of matches, while **left** and **right** joins respectively prioritise retention of rows from the left or right tables in the join. A **cross** join includes output for every pairwise combination of the left and right column values on which the join is based (it's extremely unlikely that you want to do this).\n",
    "\n",
    "We can see these outcomes reflected in the size of the tables that result from the different options which we set using the `how` parameter. This is easier to follow if we only apply the first join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97674ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"inner\", \"outer\", \"left\", \"right\", \"cross\"]\n",
    "for method in methods:\n",
    "    print(f\"{method}: {schedule.merge(airports_nz.add_suffix('_1'), how = method).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855d0c0",
   "metadata": {},
   "source": [
    "In this case the default `inner` join is exactly what we want. You may often want a `how = \"left\"` to prioritise retaining all records in your original dataset, even if no match is found in the data to be joined. The other options are less likely to be useful but it is good to be aware of them! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46cf3c5",
   "metadata": {},
   "source": [
    "## Concatenating with `concat()`\n",
    "Sometimes you have data from more than one source that are organised identically&mdash;in particular they have the same or shared column names. Less often you may have data from two different sources that record different attributes for the same set of objects, arranged in the same order. In either of these cases you can basically 'sticky-tape' the tables or series together using `concat`. We already saw this in action [back here](02-navigating-pandas.ipynb#dataframe). As usual, you can combine data row-wise or column-wise. When you want to make up a `DataFrame` from a bunch of `Series` you do `pd.concat(<list of Series>, axis = \"columns\")` but sometimes you just want to extend `Series` by appending them to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f2bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series(range(5), index = list(\"abcde\"))\n",
    "s2 = pd.Series(range(6, 11), index = list(\"fghij\"))\n",
    "print(pd.concat([s1, s2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd3649e",
   "metadata": {},
   "source": [
    "If we concatenate these two series by columns, there will be many missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64285f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([s1, s2], axis = \"columns\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda18722",
   "metadata": {},
   "source": [
    "The most likely use for this method is if you have data for a number of different places organised in similar tables and you want to combine them into single table. For example, say we had New Zealand and Australia airport data. We already have the New Zealand airports in the `airports_nz` dataframe. Say we also have an `airports_au` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa103744",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_au = airports_all[(airports_all.iso_country == \"AU\") &\n",
    "                           (airports_all.iata_code.notna())]\n",
    "airports_au = airports_au[\n",
    "   [\"name\", \"iata_code\", \"latitude_deg\", \"longitude_deg\"]] \\\n",
    "   .rename(columns = {\"latitude_deg\": \"lat\", \"longitude_deg\": \"lon\"})\n",
    "airports_au"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a97ef",
   "metadata": {},
   "source": [
    "Then we can stick them together with `concat`. This is not (for reasons unclear) a `DataFrame` method, but a `pandas` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7470f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([airports_nz, airports_au])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da01e6",
   "metadata": {},
   "source": [
    "It's worth noting here that the Australian airport data columns are in a different order, but that `pandas` finds the matching names and lines them up.\n",
    "\n",
    "If the column names in the two dataframes don't match then you get lots of NA values in the combined table because `pandas` is not psychic and doesn't know that two related names reference the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffef12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_au = airports_all[(airports_all.iso_country == \"AU\") &\n",
    "                           (airports_all.iata_code.notna())]\n",
    "airports_au = airports_au[\n",
    "    [\"name\", \"latitude_deg\", \"longitude_deg\", \"iata_code\"]]\n",
    "pd.concat([airports_nz, airports_au])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712ca99",
   "metadata": {},
   "source": [
    "And things get even weirder if you concatenate column-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477f3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([airports_nz, airports_au], axis = \"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1872c710",
   "metadata": {},
   "source": [
    "In sum, while you can use `pd.concat` to assemble big tables from small ones, you need to do this with some care, making sure that the structure of the two (or more tables are compatible). The most likely use-case for this is recombining results of some tabular data processing applied to data one subgroup at a time. This is something we look at in the next notebook. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-python-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
